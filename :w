import typing
from collections import Counter

import datasets
import pydantic_settings as pyds
import torch
from torch.utils.data import DataLoader
from transformers import AutoTokenizer

from zipf_diffusion.dit import DiT, DiTConfig


class TrainConfig(pyds.BaseSettings):
    lr: float
    max_steps: int
    batch_size: int
    dit_config: DiTConfig
    dataset_name: str = "tiny_shakespeare"
    num_workers: int = 0
    tokenizer_name: str = "gpt2"
    grad_norm: float = 1.0
    chunk_size: int
    zipf_lower_bound: float
    blank_is_noise: bool


class ZipfDataset(torch.utils.data.Dataset):
    def __init__(
        self, dataset_name: str, tokenizer_name: str, chunk_size: int, split: str
    ) -> None:
        super().__init__()

        dataset = datasets.load_dataset(dataset_name, trust_remote_code=True)[  # type: ignore
            split
        ]
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)

        def tokenize_function(examples: dict[str, typing.Any]) -> dict[str, list[int]]:
            return self.tokenizer(examples["text"])

        dataset = dataset.map(tokenize_function, batched=True)  # type: ignore

        all_tokens = []
        # Remove the zip() which is causing the unwanted dimension
        for tokens in dataset["input_ids"]:
            for i in range(0, len(tokens) - chunk_size + 1, chunk_size):
                token_chunk = tokens[i : i + chunk_size]
                if len(token_chunk) == chunk_size:
                    all_tokens.append(token_chunk)

        self.tokenized_dataset = datasets.Dataset.from_dict({"input_ids": all_tokens})

    def __len__(self) -> int:
        return len(self.tokenized_dataset)

    def __getitem__(self, index: int) -> dict:
        out = self.tokenized_dataset[index]
        out["input_ids"] = torch.tensor(out["input_ids"]).long()
        return out


class TrainState(typing.NamedTuple):
    model: DiT
    optimizer: torch.optim.AdamW
    dataset: ZipfDataset


def init_train_state(config: TrainConfig):
    model = DiT(config.dit_config)
    optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr)

    dataset = ZipfDataset(
        config.dataset_name,
        tokenizer_name=config.tokenizer_name,
        chunk_size=config.chunk_size,
        split="train",
    )
    return TrainState(model=model, optimizer=optimizer, dataset=dataset)


def create_batched_cross_reference_mask(
    reference_idxs: list[list[int]], target_tensor: torch.Tensor
) -> torch.Tensor:
    batch_size = target_tensor.shape[0]
    mask = torch.zeros_like(target_tensor, dtype=torch.bool)
    for i in range(batch_size):
        mask[i] = torch.isin(target_tensor[i], torch.tensor(reference_idxs[i]))

    return mask


def calculate_zipf_distribution(dataset: ZipfDataset) -> list[int]:
    """Calculate Zipf distribution from dataset."""
    all_ids = [
        token_id for example in dataset for token_id in example["input_ids"].tolist()
    ]
    counts = Counter(all_ids)
    sorted_counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)
    sorted_token_idxs = [item[0] for item in sorted_counts]
    return sorted_token_idxs


def log_transform(
    noise: torch.Tensor, num_tokens: int, lower_bound: float
) -> torch.Tensor:
    """Transform noise value to token index based on Zipf distribution."""
    device = noise.device

    normalized = (
        torch.log10(torch.clamp(noise, min=1e-10))
        - torch.log10(torch.tensor(lower_bound))
    ) / (-torch.log10(torch.tensor(lower_bound)))
    idx = (normalized * (num_tokens - 1)).long()
    return torch.clamp(idx, min=0, max=num_tokens - 1)


def compute_loss(
    model: DiT,
    input_ids: torch.Tensor,
    sorted_token_idxs: list[int],
    lower_bound: float,
    blank_is_noise: bool,
) -> torch.Tensor:
    time = torch.rand(input_ids.size(0))

    # Convert time to indices in the Zipf distribution
    zipf_indices = log_transform(
        time, num_tokens=len(sorted_token_idxs), lower_bound=lower_bound
    )

    # get the noise tokens for each timestep
    noise_idxs = [sorted_token_idxs[-i:] for i in zipf_indices]
    mask = create_batched_cross_reference_mask(noise_idxs, target_tensor=input_ids)

    # create noised input by replacing masked tokens
    noised_input = input_ids.clone()
    if blank_is_noise:
        noise = torch.full(
            noised_input.shape, fill_value=model.config.vocab_size - 1, dtype=torch.long
        )
    else:
        noise = torch.randint(
            low=0, high=model.config.vocab_size, size=noised_input.shape
        )

    noised_input[mask] = noise[mask]

    # Get model predictions
    pred_logits = model(
        noised_input, time=time.unsqueeze(1)
    )  # Shape: [batch_size, seq_len, vocab_size]

    loss = torch.nn.functional.cross_entropy(
        pred_logits.view(
            -1, pred_logits.size(-1)
        ).float(),  # Reshape to [batch_size * sequence_length, vocab_size]
        input_ids.view(-1),  # Reshape to [batch_size * sequence_length]
        reduction="mean",
    )
    return loss


def train_step(
    input_ids: torch.Tensor,
    sorted_token_idxs: list[int],
    state: TrainState,
    config: TrainConfig,
) -> dict:
    loss = compute_loss(
        state.model,
        input_ids=input_ids,
        sorted_token_idxs=sorted_token_idxs,
        lower_bound=config.zipf_lower_bound,
        blank_is_noise=config.blank_is_noise,
    )

    state.optimizer.zero_grad()
    loss.backward()
    grad_norm = torch.nn.utils.clip_grad_norm_(
        state.model.parameters(), config.grad_norm
    )
    state.optimizer.step()

    return dict(loss=loss.item(), grad_norm=grad_norm.item())


def generate_text(model: DiT, batch_size: int, blank_is_noise: bool, num_steps: int):
    sequence_length = model.config.sequence_length

    # create noised input by replacing masked tokens
    shape = (batch_size, sequence_length)
    if blank_is_noise:
        noise = torch.full(
            shape, fill_value=model.config.vocab_size - 1, dtype=torch.long
        )
    else:
        noise = torch.randint(low=0, high=model.config.vocab_size, size=shape)

    ts = torch.linspace(1.0, 0.0, steps=num_steps):
    for t in ts:
        x0 = model(noise, time=t.unsqueeze(1))



def train(config: TrainConfig):
    """Main training function."""
    # Load and prepare data
    state = init_train_state(config)
    sorted_token_idxs = calculate_zipf_distribution(state.dataset)

    # Create dataloader
    dl = DataLoader(state.dataset, batch_size=config.batch_size, shuffle=True)  # type: ignore

    # Training loop
    step = 0
    while step < config.max_steps:
        for batch in dl:
            if step >= config.max_steps:
                break

            input_ids = batch["input_ids"]

            info = train_step(
                input_ids,
                sorted_token_idxs=sorted_token_idxs,
                state=state,
                config=config,
            )

            print(info)

            step += 1

            if step % 100 == 0:
                print(f"Step {step}/{config.max_steps}")


def save_model(model: DiT, path: str) -> None:
    """Save the trained model."""
    torch.save(model.state_dict(), path)


if __name__ == "__main__":
    chunk_size = 128
    train_config = TrainConfig(
        lr=1e-4,
        max_steps=100,
        batch_size=4,
        dit_config=DiTConfig(
            vocab_size=65536,
            hidden_dim=16,
            num_heads=8,
            num_layers=1,
            sequence_length=chunk_size,
        ),
        chunk_size=chunk_size,
        blank_is_noise=False,
        zipf_lower_bound=1e-11,
    )

    train(train_config)
